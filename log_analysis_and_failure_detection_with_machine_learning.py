# -*- coding: utf-8 -*-
"""Log Analysis and Failure Detection with Machine Learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HZzmyR9MJYup66lN4Gd0KHJv-T5Xi0gt
"""

import pandas as pd
df = pd.read_csv('Zookeeper.log', sep="INFO",header=None,error_bad_lines=False)
print(df)

df

df.columns=['Date&Time','Message']

df

df=df.sample(25000)

import string
import nltk
from nltk.corpus import stopwords
from nltk import PorterStemmer

nltk.download('stopwords')

STOPWORDS=stopwords.words("english") #stopwords are the most common unnecessary words. eg is, he, that, etc.

def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

import string   
import re

def clean_text(text):
    ps=PorterStemmer()
    
    text=deEmojify(text) # remove emojis
    text_cleaned="".join([x for x in text if x not in string.punctuation]) # remove punctuation

    text_cleaned=re.sub(' +', ' ', text_cleaned) # remove extra white spaces
    text_cleaned=text_cleaned.lower() # converting to lowercase
    tokens=text_cleaned.split(" ")
    tokens=[token for token in tokens if token not in STOPWORDS] # Taking only those words which are not stopwords
    stemmed=[ps.stem(token) for token in tokens]
    text_cleaned=" ".join([ps.stem(token) for token in tokens])
    return text_cleaned

df['Message']=df['Message'].apply(str)

df['cleaned_comments']=df['Message'].apply(lambda x:clean_text(x))

from collections import Counter
dt=df['Message']
p = Counter(" ".join(dt).split()).most_common(50)
rslt = pd.DataFrame(p, columns=['Word', 'Frequency'])
print(rslt)

import matplotlib.pyplot as plt
import seaborn as sns

!pip install wordcloud

X=df[['cleaned_comments']]

X

X_data=X.iloc[:,:].values

X_data

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=50,ngram_range=(2,2))
X_data_vectored = vectorizer.fit_transform(X_data.ravel())

X_data_vectored

!pip install sparse

X_data_array=X_data_vectored.toarray()

X_data_array

X_data_array.shape

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X_data_array)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 10), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""***Method 1 ~ K Measn Clustering***"""

kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X_data_array)

"""**Labels**"""

print(y_kmeans)

"""***Method 2 ~ Fuzzy Clustering***"""

!pip install sklearn_extensions

from sklearn_extensions.fuzzy_kmeans import FuzzyKMeans
fuzzy_kmeans = FuzzyKMeans(k=3, m=2)
fuzzy_kmeans.fit(X_data_array)

fuzzy_kmeans.labels_

"""***Method 3 ~ Gaussian Mixture Models***"""

import numpy as np
from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=5, random_state=0).fit(X_data_array)

y_gmm=gm.predict(X_data_array)

y_gmm

"""***Method 5 ~ Agglomerative Clustering***"""

from sklearn.cluster import AgglomerativeClustering

clustering = AgglomerativeClustering().fit(X_data_array)

clustering.labels_

"""***Method 6 ~ AC - Complete***"""

#AC-complete

clustering_C = AgglomerativeClustering(linkage='complete').fit(X_data_array)

clustering_C.labels_

"""***Method 7 ~ Robust Continuous Clustering***"""

import prrcc
import numpy as np
from sklearn.metrics import adjusted_mutual_info_score

clusterer = prrcc.RccCluster(k=5,measure='cosine')

P = clusterer.fit(X_data_array)

P

"""***Method 8 ~ BIRCH Clustering***"""

#BIRCH

from sklearn.cluster import Birch
brc = Birch(n_clusters=5)
brc.fit(X_data_array)
y_pred=brc.predict(X_data_array)

y_pred

"""***Method 9 ~ DBSCAN Clustering***"""

#DBSCAN

from sklearn.cluster import DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X_data_array)
labels = db.labels_

labels

n_noise_ = list(labels).count(-1)

n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)

n_clusters_

"""**Method 10 ~ RCC with DR ( PCA)**"""

X_data_array

X_data_array.shape

from sklearn.decomposition import PCA
pca = PCA(n_components=10)
X_data_array_DR=pca.fit_transform(X_data_array)

X_data_array_DR.shape

import prrcc
import numpy as np
from sklearn.metrics import adjusted_mutual_info_score

clusterer = prrcc.RccCluster(measure='cosine')

P = clusterer.fit(X_data_array_DR)

P

"""**Streamming Data Clustering with Robust Continuous Clustering with DR**"""

import pandas as pd
df_test = pd.read_csv('server.log', sep="INFO",header=None,error_bad_lines=False)

df_test

df_test.columns=['Date&Time','Message']

df_test

df_test['Message']=df_test['Message'].apply(str)

df_test['cleaned_comments']=df_test['Message'].apply(lambda x:clean_text(x))

X_test_data=df[['cleaned_comments']]

X_data=X_test_data.iloc[:,:].values

X_data

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=100,ngram_range=(1,1))
X_data_vectored = vectorizer.fit_transform(X_data.ravel())

X_data_array_test=X_data_vectored.toarray()

X_data_array_test

X_data_array_test[[1]]

X_data_array_test[1][1]

log_list=df_test['Message']

for i in range(len(log_list)):
  print('Processsing Log ',log_list[i])
  y=kmeans.fit_predict(X_data_array_test[].reshape(-1,1))
  print('The currrent log streammed and clustered into group of ',y)